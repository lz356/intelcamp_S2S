{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedures:\n",
    "1. Install Python environment correctly (following the documentation in the repository) and run the Jupyter Notebook `S2S/S2S_Self_Defined_Data_GitHub.ipynb.ipynb`\n",
    "2. Answer the following questions: 2.1 Please describe the mechanism of the algorithm and steps of the code. ~~2.2 How does the algorithm deal with and quantify model uncertainties? 2.3 What can you do to improve the modelâ€™s accuracy?~~\n",
    "~~3. Coding Task 1: Modify/Tune the model parameter to achieve higher accuracy~~\n",
    "~~4. Coding Task 2: Apply better feature engineering to achieve higher accuracy~~\n",
    "~~5. Coding Task 3: Select a machine learning algorithm by yourself and compare its performance with this model.~~\n",
    "~~6. Write a short and brief summary of the coding and what you learned from this practice~~\n",
    "\n",
    "Follow the procedures in this jupyter Notebook and save it as a new file named \"Interview_Your Name.ipnyb\". Send it back to Dr. Liang Zhang at liangzhang1@arizona.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter as sg_filter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = \"self_defined\"\n",
    "\n",
    "# Load training dataset\n",
    "dataset_training = pd.read_csv(\"data/self_defined_training_dataset.csv\")\n",
    "\n",
    "# Load testing dataset\n",
    "dataset_testing = pd.read_csv(\"data/self_defined_testing_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Task 2: Apply better feature engineering to achieve higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing training data (train_source) and testing data (test_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SOURCE_SIZE = 3\n",
    "WINDOW_TARGET_SIZE = 3\n",
    "\n",
    "train_source = dataset_training.astype(np.float32).values\n",
    "test_source = dataset_testing.astype(np.float32).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three types of S2S model structures with different attention model\n",
    "class `S2S_Model`: vanilla S2S model without attention model\n",
    "\n",
    "class `S2S_BA_Model`: S2S model with Bahdanau Attention model (https://machinelearningmastery.com/the-bahdanau-attention-mechanism/)\n",
    "\n",
    "class `S2S_LA_Model`: S2S model with Luong Attention module (https://machinelearningmastery.com/the-luong-attention-mechanism/). `attn` module is part of `S2S_LA_Model`\n",
    "\n",
    "Each model structure can select RNN cell type, including `lstm`, `RNN`, `gru`\n",
    "\n",
    "Three model structures are parallel. Each run defines one model structure. The selection of model structure is only used for sensitivity analysis and comparison study. In the future when we decide one model structure, we can delete the rest options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "# building the S2S model\n",
    "class S2S_Model(nn.Module):\n",
    "    def __init__(self, cell_type, input_size, hidden_size, use_cuda):\n",
    "        super(S2S_Model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        if self.cell_type not in ['rnn', 'gru', 'lstm']:\n",
    "            raise ValueError(self.cell_type, \" is not an appropriate cell type. Please select one of rnn, gru, or lstm.\")\n",
    "        if self.cell_type == 'rnn':\n",
    "            self.Ecell = nn.RNNCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.RNNCell(1, self.hidden_size)\n",
    "        if self.cell_type == 'gru':\n",
    "            self.Ecell = nn.GRUCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.GRUCell(1, self.hidden_size)\n",
    "        if self.cell_type == 'lstm':\n",
    "            self.Ecell = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.LSTMCell(1, self.hidden_size)\n",
    "\n",
    "        self.lin_usage = nn.Linear(self.hidden_size, 1)\n",
    "        self.use_cuda = use_cuda\n",
    "        self.init()\n",
    "\n",
    "    # function to intialize weight parameters. Refer to Saxe at al. paper that explains why to use orthogonal init weights\n",
    "    def init(self):\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            for p in self.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    init.orthogonal_(p.data, gain=1.0)\n",
    "                if p.dim() == 1:\n",
    "                    init.constant_(p.data, 0.0)\n",
    "        elif self.cell_type == 'lstm':\n",
    "            for p in self.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    init.orthogonal_(p.data, gain=1.0)\n",
    "                if p.dim() == 1:\n",
    "                    init.constant_(p.data, 0.0)\n",
    "                    init.constant_(p.data[self.hidden_size:2*self.hidden_size], 1.0)\n",
    "\n",
    "    def consume(self, x):\n",
    "        # encoder forward function\n",
    "        # for rnn and gru\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            h = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            if self.use_cuda:\n",
    "                h = h.cuda()\n",
    "            for T in range(x.shape[1]):\n",
    "                h = self.Ecell(x[:, T, :], h)\n",
    "            pred_usage = self.lin_usage(h)\n",
    "        # for lstm\n",
    "        elif self.cell_type == 'lstm':\n",
    "            h0 = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            c0 = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            if self.use_cuda:\n",
    "                h0 = h0.cuda()\n",
    "                c0 = c0.cuda()\n",
    "            h = (h0, c0)\n",
    "            for T in range(x.shape[1]):\n",
    "                h = self.Ecell(x[:, T, :], h)\n",
    "            pred_usage = self.lin_usage(h[0])\n",
    "        return pred_usage, h\n",
    "\n",
    "    def predict(self, pred_usage, h, target_length):\n",
    "        # decoder forward function\n",
    "        preds = []\n",
    "        # for rnn and gru\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            for step in range(target_length):\n",
    "                h = self.Dcell(pred_usage, h)\n",
    "                pred_usage = self.lin_usage(h)\n",
    "                preds.append(pred_usage.unsqueeze(1))\n",
    "            preds = torch.cat(preds, 1)\n",
    "        # for lstm\n",
    "        elif self.cell_type == 'lstm':\n",
    "            for step in range(target_length):\n",
    "                h = self.Dcell(pred_usage, h)\n",
    "                pred_usage = self.lin_usage(h[0])\n",
    "                preds.append(pred_usage.unsqueeze(1))\n",
    "            preds = torch.cat(preds, 1)\n",
    "        return preds\n",
    "\n",
    "#########################################################################################\n",
    "# Bahdanau Attention model\n",
    "# refer to : AuCson github code\n",
    "# building the model\n",
    "class S2S_BA_Model(nn.Module):\n",
    "    def __init__(self, cell_type, input_size, hidden_size, use_cuda):\n",
    "        super(S2S_BA_Model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_cuda = use_cuda\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        if self.cell_type not in ['rnn', 'gru', 'lstm']:\n",
    "            raise ValueError(self.cell_type, \" is not an appropriate cell type. Please select one of rnn, gru, or lstm.\")\n",
    "        if self.cell_type == 'rnn':\n",
    "            self.Ecell = nn.RNNCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.RNNCell(1+self.hidden_size, self.hidden_size)\n",
    "        if self.cell_type == 'gru':\n",
    "            self.Ecell = nn.GRUCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.GRUCell(1+self.hidden_size, self.hidden_size)\n",
    "        if self.cell_type == 'lstm':\n",
    "            self.Ecell = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.LSTMCell(1+self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.Wattn_energies = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.Wusage = nn.Linear(self.hidden_size, 1)\n",
    "        self.Wout = nn.Linear(1+self.hidden_size*2, self.hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(self.hidden_size))\n",
    "        stdv = 1./math.sqrt(self.v.size(0))\n",
    "        self.v.data.normal_(mean=0, std=stdv)\n",
    "        self.init()\n",
    "\n",
    "# function to intialize weight parameters\n",
    "    def init(self):\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            for p in self.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    init.orthogonal_(p.data, gain=1.0)\n",
    "                if p.dim() == 1:\n",
    "                    init.constant_(p.data, 0.0)\n",
    "        elif self.cell_type == 'lstm':\n",
    "            for p in self.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    init.orthogonal_(p.data, gain=1.0)\n",
    "                if p.dim() == 1:\n",
    "                    init.constant_(p.data, 0.0)\n",
    "                    init.constant_(p.data[self.hidden_size:2*self.hidden_size], 1.0)\n",
    "\n",
    "    def consume(self, x):\n",
    "        # for rnn and gru\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            # encoder forward function\n",
    "            h = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            encoder_outputs = torch.zeros(x.shape[1], x.shape[0], self.hidden_size)\n",
    "            if self.use_cuda:\n",
    "                h = h.cuda()\n",
    "                encoder_outputs = encoder_outputs.cuda()\n",
    "            # encoder part\n",
    "            for T in range(x.shape[1]):\n",
    "                h = self.Ecell(x[:, T, :], h)\n",
    "                encoder_outputs[T] = h\n",
    "            pred_usage = self.Wusage(h)\n",
    "        # for lstm\n",
    "        elif self.cell_type == 'lstm':\n",
    "            h0 = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            c0 = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            encoder_outputs = torch.zeros(x.shape[1], x.shape[0], self.hidden_size)\n",
    "            if self.use_cuda:\n",
    "                h0 = h0.cuda()\n",
    "                c0 = c0.cuda()\n",
    "                encoder_outputs = encoder_outputs.cuda()\n",
    "            h = (h0, c0)\n",
    "            for T in range(x.shape[1]):\n",
    "                h = self.Ecell(x[:, T, :], h)\n",
    "                encoder_outputs[T] = h[0]\n",
    "            pred_usage = self.Wusage(h[0])\n",
    "        return pred_usage, h, encoder_outputs\n",
    "\n",
    "    def predict(self, pred_usage, h, encoder_outputs, target_length):\n",
    "        # decoder with attention function\n",
    "        preds = []\n",
    "        # for rnn and gru\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            for step in range(target_length):\n",
    "                h_copies = h.expand(encoder_outputs.shape[0], -1, -1)\n",
    "                energies = torch.tanh(self.Wattn_energies(torch.cat((h_copies, encoder_outputs), 2)))\n",
    "                score = torch.sum(self.v * energies, dim=2)\n",
    "                attn_weights = score.t()\n",
    "                attn_weights = torch.softmax(attn_weights, dim=1).unsqueeze(1)\n",
    "                context = attn_weights.bmm(encoder_outputs.transpose(0,1)).squeeze(1)\n",
    "                gru_input = torch.cat((pred_usage, context), 1)\n",
    "                h = self.Dcell(gru_input, h)\n",
    "                output = self.Wout(torch.cat((pred_usage, h, context), 1))\n",
    "                pred_usage = self.Wusage(output)\n",
    "                preds.append(pred_usage.unsqueeze(1))\n",
    "            preds = torch.cat(preds, 1)\n",
    "        # for lstm\n",
    "        elif self.cell_type == 'lstm':\n",
    "            for step in range(target_length):\n",
    "                h_copies = h[0].expand(encoder_outputs.shape[0], -1, -1)\n",
    "                energies = torch.tanh(self.Wattn_energies(torch.cat((h_copies, encoder_outputs), 2)))\n",
    "                score = torch.sum(self.v * energies, dim=2)\n",
    "                attn_weights = score.t()\n",
    "                attn_weights = torch.softmax(attn_weights, dim=1).unsqueeze(1)\n",
    "                context = attn_weights.bmm(encoder_outputs.transpose(0,1)).squeeze(1)\n",
    "                gru_input = torch.cat((pred_usage, context), 1)\n",
    "                h = self.Dcell(gru_input, h)\n",
    "                output = self.Wout(torch.cat((pred_usage, h[0], context), 1))\n",
    "                pred_usage = self.Wusage(output)\n",
    "                preds.append(pred_usage.unsqueeze(1))\n",
    "            preds = torch.cat(preds, 1)\n",
    "        return preds\n",
    "\n",
    "#############################################################################################3\n",
    "# Luong Attention module\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        \n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \" is not an appropriate attention method, please select one of dot, general, or concat.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        if self.method == 'concat':\n",
    "            self.attn = nn.Linear(2*self.hidden_size, self.hidden_size)\n",
    "            self.v = nn.Parameter(torch.rand(self.hidden_size))\n",
    "            stdv = 1./math.sqrt(self.v.size(0))\n",
    "            self.v.data.normal_(mean=0, std=stdv)\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        attn_energies = torch.sum(hidden*encoder_output, dim=2)\n",
    "        return attn_energies\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        attn_energies = torch.sum(hidden*energy, dim=2)\n",
    "        return attn_energies\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden.expand(encoder_output.shape[0], -1, -1),\n",
    "                            encoder_output), 2)))\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # calculate the attention weights (energies) based on the given method\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        if self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        attn_weights = torch.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "        return attn_weights\n",
    "\n",
    "#########################################################################################\n",
    "#  building the S2S LA model\n",
    "class S2S_LA_Model(nn.Module):\n",
    "    def __init__(self, cell_type, attn_method, input_size, hidden_size, use_cuda):\n",
    "        super(S2S_LA_Model, self).__init__()\n",
    "        self.cell_type = cell_type\n",
    "        self.attn_method = attn_method\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.cell_type == 'rnn':\n",
    "            self.Ecell = nn.RNNCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.RNNCell(1, self.hidden_size)\n",
    "        if self.cell_type == 'gru':\n",
    "            self.Ecell = nn.GRUCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.GRUCell(1, self.hidden_size)\n",
    "        if self.cell_type == 'lstm':\n",
    "            self.Ecell = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "            self.Dcell = nn.LSTMCell(1, self.hidden_size)\n",
    "\n",
    "        self.lin_usage = nn.Linear(hidden_size, 1)\n",
    "        self.lin_concat = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.attn = Attn(self.attn_method, self.hidden_size)\n",
    "        self.use_cuda = use_cuda\n",
    "        self.init()\n",
    "\n",
    "    # function to intialize weight parameters\n",
    "    def init(self):\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            for p in self.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    init.orthogonal_(p.data, gain=1.0)\n",
    "                if p.dim() == 1:\n",
    "                    init.constant_(p.data, 0.0)\n",
    "        elif self.cell_type == 'lstm':\n",
    "            for p in self.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    init.orthogonal_(p.data, gain=1.0)\n",
    "                if p.dim() == 1:\n",
    "                    init.constant_(p.data, 0.0)\n",
    "                    init.constant_(p.data[self.hidden_size:2*self.hidden_size], 1.0)\n",
    "\n",
    "    def consume(self, x):\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            # encoder forward function\n",
    "            h = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            encoder_outputs = torch.zeros(x.shape[1], x.shape[0], self.hidden_size)\n",
    "            if self.use_cuda:\n",
    "                h = h.cuda()\n",
    "                encoder_outputs = encoder_outputs.cuda()\n",
    "            # encoder part\n",
    "            for T in range(x.shape[1]):\n",
    "                h = self.Ecell(x[:, T, :], h)\n",
    "                encoder_outputs[T] = h\n",
    "            pred_usage = self.lin_usage(h)\n",
    "        elif self.cell_type == 'lstm':\n",
    "            h0 = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            c0 = torch.zeros(x.shape[0], self.hidden_size)\n",
    "            encoder_outputs = torch.zeros(x.shape[1], x.shape[0], self.hidden_size)\n",
    "            if self.use_cuda:\n",
    "                h0 = h0.cuda()\n",
    "                c0 = c0.cuda()\n",
    "                encoder_outputs = encoder_outputs.cuda()\n",
    "            h = (h0, c0)\n",
    "            for T in range(x.shape[1]):\n",
    "                h = self.Ecell(x[:, T, :], h)\n",
    "                encoder_outputs[T] = h[0]\n",
    "            pred_usage = self.lin_usage(h[0])\n",
    "        return pred_usage, h, encoder_outputs\n",
    "\n",
    "    def predict(self, pred_usage, h, encoder_outputs, target_length):\n",
    "        # decoder with attention function\n",
    "        preds = []\n",
    "        if self.cell_type == 'rnn' or self.cell_type == 'gru':\n",
    "            for step in range(target_length):\n",
    "                h = self.Dcell(pred_usage, h)\n",
    "                attn_weights = self.attn(h, encoder_outputs)\n",
    "                context = attn_weights.bmm(encoder_outputs.transpose(0,1))\n",
    "                context = context.squeeze(1)\n",
    "                concat_input = torch.cat((h, context), 1)\n",
    "                concat_output = torch.tanh(self.lin_concat(concat_input))\n",
    "                pred_usage = self.lin_usage(concat_output)\n",
    "                preds.append(pred_usage.unsqueeze(1))\n",
    "            preds = torch.cat(preds, 1)\n",
    "        elif self.cell_type == 'lstm':\n",
    "            for step in range(target_length):\n",
    "                h = self.Dcell(pred_usage, h)\n",
    "                attn_weights = self.attn(h[0], encoder_outputs)\n",
    "                context = attn_weights.bmm(encoder_outputs.transpose(0,1))\n",
    "                context = context.squeeze(1)\n",
    "                concat_input = torch.cat((h[0], context), 1)\n",
    "                concat_output = torch.tanh(self.lin_concat(concat_input))\n",
    "                pred_usage = self.lin_usage(concat_output)\n",
    "                preds.append(pred_usage.unsqueeze(1))\n",
    "            preds = torch.cat(preds, 1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main function\n",
    "\n",
    "The `S2S_Main` function process the data into three-dimensional for S2S model, train the model, and test the restuls\n",
    "\n",
    "- def `generate_windows`: an important procedure to convert 2-dimensional data into 3-dimensional for modeling\n",
    "\n",
    "- def `quantile_loss`: loss function with quntile number as parameter. For now, it cannot support list of quantiles as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "def S2S_Main(seed, cuda, cell_type, attention_model, la_method, window_source_size,\n",
    "            window_target_size, epochs, batch_size, hs, save_model, loss_function_qs, training_overlap, train_source, test_source):\n",
    "    t0 = time.time()\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # loss function, qs here is an integer, not a list of integer\n",
    "    def quantile_loss(output, target, qs, window_target_size):\n",
    "        \"\"\"\n",
    "        Computes loss for quantile methods.\n",
    "        :param output: (Tensor)\n",
    "        :param target: (Tensor)\n",
    "        :param qs: (int)\n",
    "        :param window_target_size: (int)\n",
    "        :return: (Tensor) Loss for this study (single number)\n",
    "        \"\"\"\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        resid = target - output\n",
    "        tau = torch.tensor([qs], device=device).repeat_interleave(window_target_size)\n",
    "\n",
    "        alpha = 0.001\n",
    "        log_term = torch.zeros_like(resid, device=device)\n",
    "        log_term[resid < 0] = torch.log(1 + torch.exp(resid[resid < 0] / alpha)) - (\n",
    "            resid[resid < 0] / alpha\n",
    "        )\n",
    "        log_term[resid >= 0] = torch.log(1 + torch.exp(-resid[resid >= 0] / alpha))\n",
    "        loss = resid * tau + alpha * log_term\n",
    "        loss = torch.mean(torch.mean(loss, 0))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # A key function to convert from 2D data to 3D data. \n",
    "    def generate_windows():\n",
    "        x_train = []\n",
    "        y_usage_train = []\n",
    "        x_test = []\n",
    "        y_usage_test = []\n",
    "\n",
    "        # for training data\n",
    "#         idxs = np.random.choice(train_source.shape[0]-(WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE), train_source.shape[0]-(WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE), replace=False)\n",
    "\n",
    "#         for idx in idxs:\n",
    "#             x_train.append(train_source[idx:idx+WINDOW_SOURCE_SIZE].reshape((1, WINDOW_SOURCE_SIZE, train_source.shape[1])) )\n",
    "#             y_usage_train.append(train_source[idx+WINDOW_SOURCE_SIZE:idx+WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE, -1].reshape((1, WINDOW_TARGET_SIZE, 1)) )\n",
    "        \n",
    "        if training_overlap == True:\n",
    "            for i in range(1,WINDOW_TARGET_SIZE+1):\n",
    "                idxs = np.arange(0, len(train_source)-(WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE), i)\n",
    "\n",
    "                for idx in idxs:\n",
    "                    x_train.append(train_source[idx:idx+WINDOW_SOURCE_SIZE, 0:-1].reshape((1, WINDOW_SOURCE_SIZE, train_source.shape[1]-1)) )\n",
    "                    y_usage_train.append(train_source[idx+WINDOW_SOURCE_SIZE:idx+WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE, -1].reshape((1, WINDOW_TARGET_SIZE, 1)) )\n",
    "\n",
    "            x_train = np.concatenate(x_train, axis=0) # make them arrays and not lists\n",
    "            y_usage_train = np.concatenate(y_usage_train, axis=0)\n",
    "\n",
    "            # for testing data\n",
    "            idxs = np.arange(0, len(test_source)-(WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE), WINDOW_TARGET_SIZE)\n",
    "\n",
    "            for idx in idxs:\n",
    "                x_test.append(test_source[idx:idx+WINDOW_SOURCE_SIZE,0:-1].reshape((1, WINDOW_SOURCE_SIZE, test_source.shape[1]-1)) )\n",
    "                y_usage_test.append(test_source[idx+WINDOW_SOURCE_SIZE:idx+WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE, -1].reshape((1, WINDOW_TARGET_SIZE, 1)) )\n",
    "\n",
    "    #         idxs = np.arange(0, len(test_source)-(WINDOW_SOURCE_SIZE+2), 1)\n",
    "\n",
    "    #         for idx in idxs:\n",
    "    #             x_test.append(test_source[idx:idx+WINDOW_SOURCE_SIZE, 0:-1].reshape((1, WINDOW_SOURCE_SIZE, test_source.shape[1]-1)) )\n",
    "    #             y_usage_test.append(test_source[idx+WINDOW_SOURCE_SIZE:idx+WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE, -1].reshape((1, WINDOW_TARGET_SIZE, 1)) )\n",
    "\n",
    "        if training_overlap == False:\n",
    "            # for training data\n",
    "            idxs = np.random.choice(train_source.shape[0]-(WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE), train_source.shape[0]-(WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE), replace=False)\n",
    "\n",
    "            for idx in idxs:\n",
    "                x_train.append(train_source[idx:idx+WINDOW_SOURCE_SIZE].reshape((1, WINDOW_SOURCE_SIZE, train_source.shape[1])) )\n",
    "                y_usage_train.append(train_source[idx+WINDOW_SOURCE_SIZE:idx+WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE, -1].reshape((1, WINDOW_TARGET_SIZE, 1)) )\n",
    "\n",
    "            x_train = np.concatenate(x_train, axis=0) # make them arrays and not lists\n",
    "            y_usage_train = np.concatenate(y_usage_train, axis=0)\n",
    "\n",
    "            # for testing data\n",
    "            idxs = np.arange(0, len(test_source)-(WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE), WINDOW_TARGET_SIZE)\n",
    "\n",
    "            for idx in idxs:\n",
    "                x_test.append(test_source[idx:idx+WINDOW_SOURCE_SIZE].reshape((1, WINDOW_SOURCE_SIZE, test_source.shape[1])) )\n",
    "                y_usage_test.append(test_source[idx+WINDOW_SOURCE_SIZE:idx+WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE, -1].reshape((1, WINDOW_TARGET_SIZE, 1)) )\n",
    "\n",
    "        x_test = np.concatenate(x_test, axis=0) # make them arrays and not lists\n",
    "        y_usage_test = np.concatenate(y_usage_test, axis=0)\n",
    "\n",
    "        return x_train, y_usage_train, x_test, y_usage_test\n",
    "\n",
    "    X_train, Y_train_usage, X_test, Y_test_usage = generate_windows()\n",
    "    print(\"Created {} train samples and {} test samples\".format(X_train.shape[0], X_test.shape[0]))\n",
    "    idxs = np.arange(0, len(test_source)-(WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE), WINDOW_TARGET_SIZE)\n",
    "    remainder = len(test_source) - (idxs[-1] + WINDOW_SOURCE_SIZE+WINDOW_TARGET_SIZE)\n",
    "    usage_actual = (np.append(train_source, test_source, axis=0))[:,-1][:-remainder]\n",
    "\n",
    "#################################################################################################################################################\n",
    "# call the model\n",
    "    #print(\"Creating model...\")\n",
    "    INPUT_SIZE = X_train.shape[-1]\n",
    "    HIDDEN_SIZE = hs\n",
    "    CELL_TYPE = cell_type\n",
    "    LA_METHOD = la_method\n",
    "\n",
    "    # call the respective model\n",
    "    if attention_model == 'none':\n",
    "        model = S2S_Model(CELL_TYPE, INPUT_SIZE, HIDDEN_SIZE, use_cuda=cuda)\n",
    "\n",
    "    elif attention_model == 'BA':\n",
    "        model = S2S_BA_Model(CELL_TYPE, INPUT_SIZE, HIDDEN_SIZE, use_cuda=cuda)\n",
    "\n",
    "    elif attention_model == 'LA':\n",
    "        model = S2S_LA_Model(CELL_TYPE, LA_METHOD, INPUT_SIZE, HIDDEN_SIZE, use_cuda=cuda)\n",
    "\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        model.cuda()\n",
    "\n",
    "    print(\"MODEL ARCHITECTURE IS: \")\n",
    "    print(model)\n",
    "\n",
    "    print(\"\\nModel parameters are on cuda: {}\".format(next(model.parameters()).is_cuda))\n",
    "\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "#     loss_fn = nn.MSELoss(reduction='sum')\n",
    "    loss_fn = quantile_loss\n",
    "    \n",
    "    EPOCHES = epochs\n",
    "    BATCH_SIZE = batch_size\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "\n",
    "    for epoch in range(EPOCHES):\n",
    "        t_one_epoch = time.time()\n",
    "        print(\"Epoch {}\".format(epoch+1))\n",
    "        total_usage_loss = 0\n",
    "        for b_idx in range(0, X_train.shape[0], BATCH_SIZE):\n",
    "\n",
    "            x = torch.from_numpy(X_train[b_idx:b_idx+BATCH_SIZE]).float()\n",
    "            y_usage = torch.from_numpy(Y_train_usage[b_idx:b_idx+BATCH_SIZE]).float()\n",
    "\n",
    "            if cuda:\n",
    "                x = x.cuda()\n",
    "                y_usage = y_usage.cuda()\n",
    "\n",
    "            # encoder forward, for respective models (with and without attention)\n",
    "            if attention_model == 'none':\n",
    "                pred_usage, h = model.consume(x)\n",
    "\n",
    "            elif attention_model == 'BA':\n",
    "                pred_usage, h, encoder_outputs = model.consume(x)\n",
    "\n",
    "            elif attention_model == 'LA':\n",
    "                pred_usage, h, encoder_outputs = model.consume(x)\n",
    "\n",
    "            # decoder forward, for respective models\n",
    "            if attention_model == 'none':\n",
    "                preds = model.predict(pred_usage, h, WINDOW_TARGET_SIZE)\n",
    "\n",
    "            elif attention_model == 'BA':\n",
    "                preds = model.predict(pred_usage, h, encoder_outputs, WINDOW_TARGET_SIZE)\n",
    "\n",
    "            elif attention_model == 'LA':\n",
    "                preds = model.predict(pred_usage, h, encoder_outputs, WINDOW_TARGET_SIZE)\n",
    "\n",
    "            # compute lose\n",
    "            loss_usage = loss_fn(preds, y_usage, qs = loss_function_qs, window_target_size=window_target_size)\n",
    "\n",
    "            # backprop and update\n",
    "            opt.zero_grad()\n",
    "\n",
    "            loss_usage.sum().backward()\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "#             total_usage_loss += loss_usage.item()\n",
    "            \n",
    "        train_loss.append(total_usage_loss)\n",
    "#         print(\"\\tTRAINING: {} total train USAGE loss.\\n\".format(total_usage_loss))\n",
    "\n",
    "#################################################################################################################################################\n",
    "# TESTING\n",
    "        y_usage = None\n",
    "        pred_usage = None\n",
    "        preds = None\n",
    "        total_usage_loss = 0\n",
    "        all_preds = []\n",
    "\n",
    "        for b_idx in range(0, X_test.shape[0], BATCH_SIZE):\n",
    "            with torch.no_grad():\n",
    "                x = torch.from_numpy(X_test[b_idx:b_idx+BATCH_SIZE])\n",
    "                y_usage = torch.from_numpy(Y_test_usage[b_idx:b_idx+BATCH_SIZE])\n",
    "\n",
    "                if cuda:\n",
    "                    x = x.cuda()\n",
    "                    y_usage = y_usage.cuda()\n",
    "\n",
    "                # encoder forward, for respective models\n",
    "                if attention_model == 'none':\n",
    "                    pred_usage, h = model.consume(x)\n",
    "\n",
    "                elif attention_model == 'BA':\n",
    "                    pred_usage, h, encoder_outputs = model.consume(x)\n",
    "\n",
    "                elif attention_model == 'LA':\n",
    "                    pred_usage, h, encoder_outputs = model.consume(x)\n",
    "\n",
    "                # decoder forward, for respective models\n",
    "                if attention_model == 'none':\n",
    "                    preds = model.predict(pred_usage, h, WINDOW_TARGET_SIZE)\n",
    "\n",
    "                elif attention_model == 'BA':\n",
    "                    preds = model.predict(pred_usage, h, encoder_outputs, WINDOW_TARGET_SIZE)\n",
    "\n",
    "                elif attention_model == 'LA':\n",
    "                    preds = model.predict(pred_usage, h, encoder_outputs, WINDOW_TARGET_SIZE)\n",
    "\n",
    "                # compute loss\n",
    "                loss_usage = loss_fn(preds, y_usage, qs = loss_function_qs, window_target_size=window_target_size)\n",
    "\n",
    "                if (epoch == epochs-1):\n",
    "                    all_preds.append(preds)\n",
    "\n",
    "        test_loss.append(total_usage_loss)\n",
    "\n",
    "#         print(\"\\tTESTING: {} total test USAGE loss\".format(total_usage_loss))\n",
    "#         print(\"\\tTESTING:\\n\")\n",
    "        print(\"\\tSample of prediction:\")\n",
    "        print(\"\\t\\t TARGET: {}\".format(y_usage[-1].cpu().detach().numpy().flatten()))\n",
    "        print(\"\\t\\t   PRED: {}\\n\\n\".format(preds[-1].cpu().detach().numpy().flatten()))\n",
    "\n",
    "        y_last_usage = y_usage[-1].cpu().detach().numpy().flatten()\n",
    "        pred_last_usage = preds[-1].cpu().detach().numpy().flatten()\n",
    "        t2_one_epoch = time.time()\n",
    "        time_one_epoch = t2_one_epoch - t_one_epoch\n",
    "        print(\"TIME OF ONE EPOCH: {} seconds and {} minutes\".format(time_one_epoch, time_one_epoch/60.0))\n",
    "\n",
    "####################################################################################################\n",
    "# SAVING MODEL\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), \"MODEL_w:__seed={}_cell_type={}_attention_model={}_la_method={}_T={}_N={}_bs={}_hs={}\".format(\n",
    "            seed, cell_type, attention_model, la_method,\n",
    "            window_source_size, window_target_size, batch_size, hs))\n",
    "\n",
    "#################################################################################################################################################\n",
    "# RESULTS\n",
    "    # for plotting and accuracy\n",
    "    preds = torch.cat(all_preds, 0)\n",
    "    preds = preds.cpu().detach().numpy().flatten()\n",
    "    actual = Y_test_usage.flatten()\n",
    "\n",
    "    # for loss plotting\n",
    "    train_loss_array = np.asarray(train_loss)\n",
    "    test_loss_array = np.asarray(test_loss)\n",
    "    len_loss = np.arange(len(train_loss_array))\n",
    "\n",
    "#     # unnormalizing 1\n",
    "#     if normalization:\n",
    "#         preds_unnorm = (preds*std_usage) + mu_usage\n",
    "#     else:\n",
    "#         preds_unnorm = preds\n",
    "\n",
    "    # using the actual usage from top of script here\n",
    "#     mae3 = (sum(abs(usage_actual - preds_unnorm)))/(len(usage_actual))\n",
    "#     mape3 = (sum(abs((usage_actual - preds_unnorm)/usage_actual)))/(len(usage_actual))\n",
    "    mae3 = 0\n",
    "    mape3 = 0\n",
    "\n",
    "    # for std\n",
    "#     mape_s = (abs((usage_actual - preds_unnorm)/usage_actual))\n",
    "    mape_s = 0\n",
    "#     s = mape_s.std()\n",
    "    s = 0\n",
    "#     mae_s = abs(usage_actual - preds_unnorm)\n",
    "    mae_s = 0\n",
    "#     s2 = mae_s.std()\n",
    "    s2 = 0\n",
    "    print(\"\\n\\tACTUAL ACC. RESULTS: MAE, MAPE: {} and {}%\".format(mae3, mape3*100.0))\n",
    "    \n",
    "    if not os.path.exists(f'results/{site}'):\n",
    "        os.makedirs(f'results/{site}')\n",
    "    \n",
    "    pd.DataFrame(preds, columns = [f'q{loss_function_qs}']).to_csv(f'results/{site}/q{loss_function_qs}.csv', index = None)\n",
    "    pd.DataFrame(actual, columns = [f'actual']).to_csv(f'results/{site}/actual.csv', index = None)\n",
    "\n",
    "    # total time of run\n",
    "    t1 = time.time()\n",
    "    total = t1-t0\n",
    "    print(\"\\nTIME ELAPSED: {} seconds OR {} minutes\".format(total, total/60.0))\n",
    "    print(\"\\nEnd of run\")\n",
    "    plt.show()\n",
    "#     for_plotting = [usage_actual, preds_unnorm, y_last_usage, pred_last_usage]\n",
    "    return s, s2, mape_s, mae_s, mae3, mape3, total/60.0, train_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training and testing\n",
    "\n",
    "for loss_function_qs in [0.05, 0.25, 0.50, 0.75, 0.95]: #[0.50]:#[0.05, 0.50, 0.95]:\n",
    "    print(f\"Processing loss function alpha: {loss_function_qs}\")\n",
    "    s, s2, mape_s, mae_s, mae, mape, total_mins, train_loss, test_loss= S2S_Main(seed=42, cuda=False,\n",
    "        cell_type='rnn', attention_model='BA', la_method='none',\n",
    "        window_source_size=WINDOW_SOURCE_SIZE, window_target_size=WINDOW_TARGET_SIZE, epochs=20,\n",
    "        batch_size=256*4, hs=64*1, save_model=False, loss_function_qs = loss_function_qs, training_overlap = True, train_source = train_source, test_source = test_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Task 1: Modify/Tune the model parameter to achieve higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self-defined data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q95 = pd.read_csv(f'results/{site}/q0.95.csv')\n",
    "q75 = pd.read_csv(f'results/{site}/q0.75.csv')\n",
    "q50 = pd.read_csv(f'results/{site}/q0.5.csv')\n",
    "q25 = pd.read_csv(f'results/{site}/q0.25.csv')\n",
    "q05 = pd.read_csv(f'results/{site}/q0.05.csv')\n",
    "actual = pd.read_csv(f'results/{site}/actual.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4.5))\n",
    "\n",
    "# calculating error\n",
    "y_actual = actual\n",
    "y_predicted = q50.values.flatten()\n",
    "MSE = mean_squared_error(y_actual, y_predicted)\n",
    "RMSE = MSE**0.5\n",
    "CV_RMSE = RMSE/y_actual.mean().values[0]\n",
    "mae = mean_absolute_error(y_actual, y_predicted)\n",
    "\n",
    "plt.title(f'{site} Whole Building Real Power Total Prediction\\nError: MAE:{mae:.2f} kW, CV-RMSE:{CV_RMSE*100:.2f}%')\n",
    "\n",
    "plt.plot(actual, label = 'actual')\n",
    "plt.plot(sg_filter(q95.values.flatten(),5,2), label = 'Q95', color = '#a2a2a2')\n",
    "plt.plot(sg_filter(q75.values.flatten(),5,2), label = 'Q75', color = '#5b5b5b')\n",
    "plt.plot(sg_filter(q50.values.flatten(),5,2), label = 'Medium', color = '#322b2b')\n",
    "plt.plot(sg_filter(q25.values.flatten(),5,2), label = 'Q25', color = '#5b5b5b')\n",
    "plt.plot(sg_filter(q05.values.flatten(),5,2), label = 'Q5', color = '#a2a2a2')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# plt.ylim(20,40)\n",
    "# n=0\n",
    "# plt.xlim(1000, 1720)\n",
    "\n",
    "plt.ylabel(f'{site} Whole Building Real Power Total')\n",
    "plt.xlabel('Timesteps, 1 min/unit, showing 33 hours results')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Task 3: Select a machine learning algorithm by yourself and compare its performance with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
